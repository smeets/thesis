%!TEX root = report.tex

\chapter{Discussion and Future Work}

In this chapter we first discuss the experiments and their results. Then we
proceed to discuss the model based on experiments. Finally, we present some
ideas on interesting research opportunities and our closing thoughts.

\section{Discussion of results}

In this section we will comment on the results obtained, primarily from our
experiments. More importantly, we will also expand upon the discussion
presented for each experiment in the Chapter 4 where we explained our
experimental approach.

\subsection{Experiment 1: Wireshark}

As revealed in Figure \ref{fig:wstiming} (actual timing model), the design of
this experiment has a critical flaw which is not apparent without deeper
insight into the network stack. As explained earlier, the ``packet tap''
feature from which Wireshark obtains packets is triggered when the packet
exits the kernel queueing system (qdisc) and handed over to the driver for
transmission, rather than once the driver receives a transmission event from
the hardware. This completely invalidates the fundamental design model and
thus the experiment as such.

Further exploration showed that the experiment design could work if packets
could be timestamped in hardware and somehow captured after transmission.
There \emph{is} support for this in Linux, and Wireshark, but unfortunately
relies on features not available in our hardware.

The presented results are mostly included for completeness and should not be
regarded as anything useful, except for measuring system call to pcap
latencies.

\subsection{Experiment 2: Queueing the Network System}

As described in Section \ref{sec:experiment2}, there are some issues with the
definition of $W_\text{NIC}$ from Equation \ref{eq:wnic} due to a deferred
memory release mechanism in the kernel. Specifically, the definition of the
NIC queuing system from Figure \ref{fig:exp2_overview} includes a note
``kernel scheduler frees handled pkts''.

In order to increase throughput (at the expense of latency), Linux will free
up handled packets in batches. This part greatly impacts the \texttt{sndbuf}
which is used in Equation \ref{eq:sndbuf}, implying that sent or dropped
packets -- but not yet freed -- will still count as enqueued in the NIC. In
short, the modeled $L_\text{NIC}$ includes packets in driver/firmware, in
hardware and, crucially, in the kernel's to-free list.

By increasing the \texttt{sndbuf} we could observe a corresponding increase in
$L_\text{NIC}$ (and $W_\text{NIC}$) in our experiments. However, this fact
does not imply that there are not any more packets in the NIC. Recall that the
NIC queueing system actually consists of a driver/firmware queue and a
hardware queue. The hardware queue on most consumer chips is \emph{probably}
either fixed and very limited in size, or non-existant (RAM \emph{is}
expensive).

After additional analysis of the source code of Intel's open source Wi-Fi
driver, the driver/firmware queue length was determined to be 256 packets
large. By assuming a full \texttt{sndbuf} (i.e. $K$ packets in the network
system) and a full NIC, we can model $K$ as $K = L_\text{qdisc} + 256 +
\text{hardware} + \text{to-be-free}$ where \texttt{hardware} and
\texttt{to-be-freed} are unknowns.

One can argue that for the hardware queue to have any impact on the other
queues, it would have to be of similar size. Otherwise, it should be safe to
assume an interrupt-based solution: a driver manages known packets and
hardware signals which have been sent using (soft) interrupt requests (IRQ),
i.e. no dedicated hardware buffer. Thus the total queue size of the NIC is
determined by the driver, in our case $256$ packets, simplifying $K =
L_\text{qdisc} + 256 + \texttt{to-be-freed}$. This exposes the core issue with
the original queue model: there is no separation between ``in queue, waiting
to be served'' and ``served, waiting to be freed''.

In the program which logged $L_\text{qdisc}$, we could observe how many
packets were fed into the driver ``at once''. Our tests show that multiple,
10+, packets being handed over to the driver between timestamps $<10 \mu s$.
This confirms that the driver is able to keep the internal buffer/queue fully
saturated, but does not shed any light on the size of \texttt{to-be-freed}.

We can estimate the range of \texttt{to-be-freed} to $[K - L_\text{qdisc}, K -
256 - L_\text{qdisc}] = [307, 563]$, depending on the amount of packets in the
NIC waiting to be transmitted. Reworking Table \ref{tab:exp2data}
($L_\texttt{NIC} = [1, 256]$) we now obtain a $W_\texttt{NIC} =
\frac{256}{\lambda_\texttt{NIC}} \approx [240~\mu s, 62~ms]$, again depending
on the amount of packets in the NIC waiting to be transmitted.

In conclusion, this experiment, under assumption of saturation, no hardware
buffer on the NIC, $L_\texttt{NIC} = 256$ packets, can estimate the range of
$W_\texttt{NIC}$. As mentioned earlier, comparing the obtained
values from our experiment with the Felemban-Ekici model is prevented by
different network speeds and configurations.

\subsection{Experiment 3: Hacking on the driver}

Since we could not find a concise definition of the
\texttt{wireless\_media\_time}, and ultimately argued that the value must include $T_{BACKOFF}$ if there is a positive correlation with regards to the number
of nodes, and include $T_{FRAME}$ if there is a linear correlation with regards to the payload size.

% TODO: Discuss wireless media time, 2G and 5G difference
The sampled \texttt{wireless\_media\_time} presented in Figure \ref{fig:exp3mediatime} (2.4 GHz) shows an interesting behaviour: the timing data from the system under test (localhost) clearly increase as the number
of active raspberry pi nodes increases. This behaviour is not apparent in the control set (values sampled from another system running
the same kernel but with another NIC), as the timing values do not strictly
increase. A statistical analysis of the datasets could determine the level of correlation.

It looks like the payload size is a contributing factor the
media time on the localhost dataset, but not on the control dataset. The relative
media time factor is around 2X while the payload size factor is between $64X$ to $8000X$, and the absolute difference is around $100~\mu s$, which amounts to about
$100~\text{$\mu$S} \times 144~\text{Mbps} = 14.4~\text{Kb}$. Thus we can immediately conclude that there is no linear correlation between payload size and
media time. Our best guess is that the NIC probably does about $100~\mu s$ worth
of work between each (s)IRQ (which is when we sample). This guess does not seem
to hold for the control set, however.

Moving on to 5G, Figure \ref{fig:exp4mediatime} presents a completely different
behavior and the only similarity with 2G is the shared minimum media time, at around $100~\mu s$. There does not seem to be any immediate correlation between either payload size and media time, nor number of nodes and media time. Even though
we label this test as inconclusive, at least both the localhost dataset and control dataset show very similar behaviour. Comparing 2G and 5G modem behaviour is very tricky, not only due to a difference in protocol but often also due
to a difference in hardware, firmware and kernel driver. It is worth noting that
the difference observed suggest that there is no silver bullet which works for
both IEEE 802.11 n (2G) and IEEE 802.11 ac (5G).

Indeed, differences between IEEE 802.11 n and IEEE 802.11 ac can have dramatic effects
on observable metrics. Recall that during the CSMA process, time is quantized into time-slots, from 9
to 50 $\mu s$. At similar physical transmission rates, with $T_{\texttt{slot}}
= 9$ will \emph{attempt} to decrement the $T_{\texttt{backoff}}$ more than 5
times as quickly, compared with $T_\texttt{slot} = 50$. However, since
transmission take exactly the same amount of time, the only difference becomes
the time spent waiting for the \texttt{DIFS} and \texttt{SIFS}. Now consider
that when $T_{\texttt{slot}} = 9$, the physical transmission rate is 200 Mbps
and 72.2 Mbps when $T_{\texttt{slot}} = 50$. This is IEEE 802.11 n compared to
IEEE 802.11 ac.

The insight here is that a IEEE 802.11 ac network can send
frames a) faster compared to n and b) decrements the backoff counter more
rapidly, with the implication that, network-wide, there will be more
transmission attempts and, consequently, packet collisions, compared to
IEEE 802.11 n, assuming network saturation and equal sample durations.

This behavior is implied in graphs, where the media time for small packets in IEEE 802.11 ac is significantly higher compared to IEEE 802.11 n (a large media time should primarily be caused by collisions and thus, by definition, frame drops). We will continue to discuss the frame drop probability in the upcoming section.

In conclusion, we found that the experiment could show expected correlation between $T_{BACKOFF}$ and media time for 802.11 n. This was, however, not
observed on our control system and therefore we have to conclude that our findings are too inconclusive, with a saving clause that the experiment shows promise.

\section{The Felemban-Ekici Model}

As it turned out, evaluating the usefulness of the Felemban-Ekici model was an
incredibly difficult endeavour. Unfortunately, the first step of the thesis --
gathering data to evaluate the model with -- turned out to be the real
challenge, and challenging problems have their own lure -- they hook you in
until they are solved, or, as in our case, time runs out.

To our disappointment, we could not discover any combination of parameters in
our re-implementation of the Felemban-Ekici model which resulted in similar
throughput and access delay results as the original paper. As seen in Figure
\ref{fig:modelimpl}, we could, however, accurately implement the conditional
packet collision $P$.

\begin{figure}[tbp]although
  \centering
  \includegraphics[width=0.8\textwidth]{images/reimpl.pdf}
  \caption{Our model reimplementation compared to values extracted from the original paper under similar network conditions.}
  \label{fig:modelimpl}
\end{figure}

To put this into perspective, at rougly 50 connected clients the conditional
packet collision probability reaches $40\%$, which results in a $0.4^{L+1}
\approx 0.06\%$ probability to fail transmission $L+1$ times and drop a
packet. At 20 clients, which is what we used during experiment 3, this falls
to $0.27^{L+1} \approx 0.003\%$.

Since clients are assumed to be identical, the packet drop probability for one
client and full network are equivalent. In Figure \ref{fig:relative_txfail_2g}
we show the total number of \texttt{tx\_fail}s (as reported to the kernel)
relative to the total number of packets transmitted, for 802.11 \emph{n}. Data
from 802.11 \emph{ac} is shown in Figure \ref{fig:relative_txfail_5g}. As seen
directly, the clients have a problem staying connected to the network throughout
the 802.11 \emph{n} test (indicated by lack of plots) and only few or no
packet drops were detected. The 802.11 \emph{ac} test, however, hovers around
the $0.003$, a full two orders of magnitude larger than the model's estimated
$0.003\%$.

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.8\textwidth]{images/relative_tx_fail_2g.pdf}
  \caption{Empirically obtained packet drop probability for 802.11 n.}
  \label{fig:relative_txfail_2g}
\end{figure}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.8\textwidth]{images/relative_tx_fail_5g.pdf}
  \caption{Empirically obtained packet drop probability for 802.11 ac.}
  \label{fig:relative_txfail_5g}
\end{figure}

Finally, we tried to rescale and normalize the throughput by assuming that the
model was true. The model estimates about that at $N = 5$, normalized
throughput is at $68\%$, which puts channel capacity at
$\frac{\text{throughput}}{0.68}$. Figures \ref{fig:rescaled_tput_2g} and
\ref{fig:rescaled_tput_5g} show the (rescaled) normalized network throughput
(using the computed channel capacity), for varying $N$ and payload size. In
both figures, the cyan curve show the normalized throughput obtained from the
Felemban-Ekici paper with 1 KB payload at 1 Mbps (over 802.11 n).

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.8\textwidth]{images/rescaled_u_2g.pdf}
  \caption{Rescaled, (measured) normalized network throughput for 802.11 n compared with Felemban-Ekici.}
  \label{fig:rescaled_tput_2g}
\end{figure}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.8\textwidth]{images/rescaled_u.pdf}
  \caption{Rescaled, (measured) normalized network throughput during 802.11 ac compared with Felemban-Ekici.}
  \label{fig:rescaled_tput_5g}
\end{figure}

Figure \ref{fig:rescaled_tput_2g} (802.11 n) shows an expected behavior. Larger
payload sizes have higher relative utilization and roughly follow the
Felemban-Ekici curve. Lower payload sizes fall behind very fast. This hints at
a weakness in the model regarding different traffic flows.

Seemingly in opposition, Figure \ref{fig:rescaled_tput_2g} appears to imply
that smaller payload sizes incur no penalty as $N$ increases. Figure
\ref{fig:exp4throughput} reveals why, the throughput is already incredibly
low. Moving on, the larger payload sizes show similar curves compared to
802.11 n.

In conclusion, we cannot determine whether the predicted conditional packet
collision probability is correct or not. Although the model appears to predict
network throughput degredation as the number of connected nodes increase, it's
certainly not an apples to apples comparison. Our 802.11n network was mostly
stable at 72.2 (for raspberri pi 4's) and 200 Mbps on 802.11 ac, quite
different than model's 1 Mbps channel. As we could not generate normalized
throughput values from our reimplementation we cannot know what the output of
the model would have been for our network configurations.


\section{Future work}

\begin{itemize}

\item As seen in the network overview (Figure \ref{fig:linux_egress}) and
noticed during experiment 2, system call overhead and the userland/kernel
split have serious implications for latency-sensative measurements. An area
worth exploring is known as ``userland networking'' -- a technique common in
high-performance packet processing in which a userland application is allowed
exclusive access of a network device. Bypassing the kernel removes potential
bottlenecks such as context switches, memory allocation and copying,
scheduling and multithreading. In theory, this technique can reduce the number
of system calls of an application. More importantly, such an application can
design the communication between NIC and userland as a ring buffer,
eliminating the cost associated with memory allocation and deallocation
experienced in Experiment 2 (the unknown number of \texttt{to-be-freed}
packets).

\item An alternative approach to bypassing the kernel, is to bring the
userland program \emph{into} the kernel. The Berkley Packet Filter (BPF) is a
register-based filter evaluator designed for packet filtering
\cite{10.5555/1267303.1267305}. It has since been extended and redesigned
under the ``extended BPF'' (eBPF) moniker as an in-kernel virtual machine with
filters, taps and hooks all over the kernel. In short, the eBPF machine can
run arbitrary code inside the kernel, triggered by specific events. In theory,
this should allow for more accurate measurement of the packet transmission
process.


\item Continuing on the started path with a modified Wi-Fi driver, it would be
useful to also perform similar tests where the \texttt{jana} server also has
to reply back to the clients. These tests should more closely mimic the TCP
protocol upon which most of today's networking rely. Measuring the round trip
time (RTT) of packets and comparing the measured values with a modeled RTT (
based on IEEE 802.11 and TCP) could also lead to some interesting results.


\item And as is common practice today: when in doubt, throw machine learning
at the problem. While machine learning cannot enable more accurate
measurements, a machine-learning model could be trained to predict network
behaviour based on a large set of collected and labeled (as in, metric X
indicate outcome Y) metrics. A model may also be constructed for determining
possible user-friendly interventions (move device closer to router, the
channel is currently observing heavy interference so switch channel), however
an expert system (guided by real-time metrics) would probably be sufficient.
The system could be designed to run on each device (distributed) or on a
trusted machine (centralised). A centralised machine would have more
information to act on when issuing interventions back to users and,
potentially, directly to the Wi-Fi routers. Such a system could easily be
extended to record the outcome of any interventions (e.g. significantly
better or worse), possibly allowing for collection of data for unsupervised
machine learning.

During our thesis work we were able to identify several interesting metrics,
available from \texttt{ubus}, such as number of connected clients,
neighbouring network data, medium availability, TX/RX PHY rates, RSSI,
glitches and RTS settings. See \url
{https://github.com/smeets/thesis/blob/master/paramlist.md} for more
parameters and their descriptions.


\end{itemize}

\section{Closing remarks \& thoughts} 

This thesis primarily indicates that it currently is and most probably will
become increasingly difficult to evaluate and use existing models for
practical Wi-Fi equipment. As the IEEE 802.11 specification evolves, new
Wi-Fi routers will have new implementations and new features. At least, there
are probably fewer chipset makers than router equipment manufacturers, so the
expected variance (of the chipset and driver) of these future devices should be
similar to today's devices.

Our attempts to obtain highly accurate packet timing data also show
that it requires a non-trivial amount of effort. It seems more likely that 
an overall measurement using ML/AI system is easier to obtain for helping 
customers with their routers than trying to develop a system that works from the 
inside out as there are too many gaps between model and physical reality.

However, there is a kind of natural elegance to the way the Bianchi
(and subsequent models) approach network performance. By focusing on the
(assumed) bottleneck–the Distributed Coordination Function–most of the recent
improvements in throughput (e.g. batching, MI/MO, multi-user MI/MO) can
simply be abstracted as channel capacity (which can be reliably sampled from
the driver). In addition to this versatile way of modelling different
performance features, the models have excellent performance in Wi-Fi
simulations. We therefore believe that is is important to construct a
functional implementation– which we weren't able to do–and run experiments in
consumer networks.
